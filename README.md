# Decompiled Smart Contract Refinement Engine

This project presents a complete, multi-stage pipeline designed to automatically refine decompiled Solidity smart contracts using Large Language Models (LLMs). The primary goal is to take low-quality, often uncompilable code generated by a decompiler and transform it into high-quality, readable, and compilable Solidity code that is logically equivalent to the original source.

The pipeline handles everything from data acquisition from the Ethereum blockchain to the final quantitative and qualitative evaluation of the LLM's performance. It is structured as a series of modular Python scripts, allowing for easy extension and experimentation with different models, decompilers, and prompt engineering strategies.

## Features

  * **End-to-End Pipeline**: Automates the entire workflow: data scraping, enrichment, decompilation, dataset creation, model inference, and evaluation.
  * **Blockchain Data Acquisition**: Scans a given range of Ethereum blocks to find and extract bytecode from newly created smart contracts.
  * **Etherscan Integration**: Enriches the raw data by fetching verified source code, ABI, and compiler metadata from the Etherscan API.
  * **Advanced Decompilation**: Utilizes the `heimdall-rs` decompiler to convert contract runtime bytecode back into Solidity-like code.
  * **Context-Aware Dataset Creation**: Intelligently parses both original and decompiled code to create a parallel dataset of function pairs. The final dataset includes rich context such as the contract's ABI, state variables, events, and inheritance structure.
  * **Flexible LLM Integration**: Supports various LLMs (e.g., GPT-o3, GPT-4o) and provides a framework for testing different prompt engineering strategies.
  * **Sophisticated Prompt Engineering**: Implements and tests multiple prompt strategies, from highly-constrained prompts that preserve logical fidelity to minimalist and advanced Chain-of-Thought prompts.
  * **Automated Quantitative Evaluation**: Provides a robust suite of scripts to automatically assess the quality of the LLM-generated code using multiple metrics:
      * **Recompilation Rate**: Checks if the refined code is syntactically correct and can be compiled.
      * **BLEU Score**: Measures n-gram similarity to the original source code.
      * **Edit Distance (Levenshtein)**: Calculates the character-level difference between the refined and original code.
      * **Semantic Similarity**: Uses sentence transformers to measure the cosine similarity of the code's semantic meaning.
  * **In-Depth Error Analysis**: Includes a script to perform a deeper, qualitative analysis by classifying common errors in the LLM's output (e.g., logic hallucination, truncation) and visualizes the results with charts and a detailed CSV report.

## Workflow Architecture

The project is structured as a sequential pipeline. Each stage produces artifacts that are consumed by the next stage.

**Stage 1: Data Collection and Preparation**

1.  **`scrape_archive_data.py`**: Connects to an Ethereum archive node, scans blocks, and saves the bytecode and transaction info for new contracts.
2.  **`enrich_with_etherscan.py`**: Takes the contract addresses from Stage 1 and queries the Etherscan API to download the verified source code and ABI.
3.  **`filter_verified_data.py`**: Cleans the dataset by selecting only contracts that were successfully verified and contain all necessary data fields.

**Stage 2: Decompilation and Dataset Creation**
4\.  **`heimdall_decompiler_final.py`**: Uses the `heimdall` command-line tool to decompile the runtime bytecode for each verified contract.
5\.  **`split_and_pair_functions_*.py`**: Parses both the original and decompiled source files. It extracts functions and matches them based on their signature. It saves these pairs along with rich contextual metadata into a `.jsonl` file.
6\.  **`split_dataset_*.py`**: Splits the final paired dataset into training, validation, and test sets for the machine learning task.

**Stage 3: LLM Refinement and Evaluation**
7\.  **`gpt4o_caller_*.py` / `gpto3_caller_*.py`**: These scripts take a dataset (e.g., the validation or test set), format the decompiled code into a specific prompt, and send it to an LLM API. The model's refined code is saved for evaluation.
8\.  **`evaluate_*.py`**: These scripts run the automated quantitative evaluation metrics (compilation, BLEU, etc.) on the output directory from the previous step, producing a final report.
9\.  **`error_analysis_gpto3_by_content.py`**: This script performs a deeper, qualitative analysis by classifying error types in a sample of the model's output and generating charts and a CSV file to provide insights into failure modes. The script identifies specific issues like `Logic hallucination`, `Truncated output`, and `Incorrect semantic substitution`.

## Getting Started

### Prerequisites

  * Python 3.8+
  * An Ethereum Archive Node URL.
  * An Etherscan API Key.
  * An OpenAI (or compatible) API Key.
  * The `heimdall-rs` decompiler installed and available in your system's PATH.
  * The Solidity compiler (`solc`). The evaluation scripts can automatically install the required version using `py-solc-x`.

### Installation

1.  **Clone the repository:**

    ```bash
    git clone https://github.com/Nancy0209/LLM-Based-Decompilation-of-EVM-Bytecode-for-Smart-Contract.git
    cd LLM-Based-Decompilation-of-EVM-Bytecode-for-Smart-Contract
    ```

2.  **Install Python dependencies:**

    ```bash
    pip install -r requirements.txt
    pip install py-solc-x sacrebleu python-Levenshtein sentence-transformers torch
    ```

3.  **Set up API Keys:**
    This project requires you to manually set the API keys in the script files. Please open the corresponding scripts and paste your key in the designated location.
    
    Etherscan API Key: Edit the enrich_with_etherscan.py and scrape_archive_data.py files.

    OpenAI API Key: Edit all gpto3_caller_*.py and gpt4o_caller_*.py files.

### Running the Pipeline

You can run the entire pipeline by executing the scripts in the correct order.

```bash
# Stage 1: Data Collection
python3 scripts/1_scraping/scrape_archive_data.py
python3 scripts/1_scraping/enrich_with_etherscan.py
python3 scripts/1_scraping/filter_verified_data.py

# Stage 2: Decompilation and Dataset Creation
python3 scripts/2_decompilation/heimdall_decompiler_final.py
python3 scripts/3_dataset_building/split_and_pair_functions_*.py
python3 scripts/3_dataset_building/split_dataset_*.py

# Stage 3: LLM Refinement (Example using GPT-4o or/and GPT-o3 with the context-enhanced prompt)
python3 scripts/4_model_inference/gpt4o_caller_*.py
or/and
python3 scripts/4_model_inference/gpto3_caller_*.py

# Stage 4: Evaluation
python3 scripts/5_evaluation/evaluate_4o_results_*.py
python3 scripts/5_evaluation/error_analysis_gpto3_by_content.py # (Adjust script to point to gpt4o results if needed)
```

## Experimentation

This codebase is designed for experimentation. You can easily test different components:

  * **To test a different model:** Modify one of the `_caller_` scripts by changing the `MODEL_NAME` and `API_KEY` variables.
  * **To test a new prompt:** Create a new `build_prompt` function in any `_caller_` script and implement your new prompt engineering strategy.
  * **To evaluate a different result set:** Change the `EVALUATION_DIRECTORY` variable in any `evaluate_*.py` script to point to the results you want to analyze.
